# Performance monitoring of SaaS solutions

Monitoring performance after contract award is just as important as selecting the right solution. In a SaaS environment, where the agency does not own or directly manage the infrastructure or application code, oversight must focus on service delivery, user adoption, and alignment with contract terms‚Äî-particularly SLAs (service level agreements).

## üéØ Focus on value delivery and user adoption

SaaS performance monitoring isn't just about technical uptime‚Äî-it's about ensuring the service is usable and reliable, and delivering its promised business value. Agencies must use tools and governance structures that allow for continuous visibility into service health and utilization.

## üõ†Ô∏è Establish SLA monitoring and vendor accountability

A core component of SaaS oversight is validating that the vendor is meeting contractual SLAs.

Key steps include:

- Define and document the monitoring methodology and tools to be used‚Äî-ideally, those integrated into the vendor's platform or via real-time Application Programming Interfaces (APIs).
- Require the vendor give a real-time performance dashboard that includes:
  - System uptime and availability metrics.
  - Incident logs with duration, cause, and user impact.
  - Planned vs. unplanned outages.
  - SLA compliance rates for issue resolution (e.g., mean time to resolution by severity level).
- Set automated alerting to notify government stakeholders of SLA breaches or degradation in service quality.
- Document procedures for remediation and escalation when SLAs are not met, including potential financial remedies or service credits.
- Conduct regular SLA review meetings with the vendor to discuss trends, issues, and improvement actions.

## üë• Monitor user adoption and utilization

High system availability means little if users are not trying out the tool or using it effectively. Assess adoption trends, user engagement, and value realization and monitor them.

Recommended monitoring areas:

- **User login frequency and distribution** across departments or roles.
- **License utilization**: active vs. assigned licenses and idle license identification. To keep costs down, don't give licenses to people who won't need them, and take them away from people who aren't using them.
- **Feature usage**: what capabilities are being used regularly vs. underused (or not used at all).
- **Support ticket trends** that may signal usability or adoption challenges.

Collect both quantitative and qualitative feedback:

- Conduct **user surveys** to measure ease of use, functionality of specific features, and overall perception of the solution.
- Hold **focus groups** or user councils to gather insights about workflow fit, pain points, and unmet needs.

## üìä Build a comprehensive performance monitoring dashboard

A centralized dashboard gives both agency stakeholders and vendors  a shared view of performance.

### Service health and availability

- Actual uptime vs. contracted SLA.
- Downtime logs with root cause and affected user groups.
- Time-of-day and frequency analysis.
- Performance during peak usage periods.

### Support services

- Support ticket volumes by type.
- Mean time to resolution by priority.
- First contact resolution rates.
- SLA compliance for resolution timelines.
- Reopened ticket rates and backlog aging.

### User adoption and satisfaction

- User adoption and trends over time.
- Net Promoter Score (NPS) and System Usability Scale (SUS).
- Training feedback and perceived effectiveness.
- Feature-specific satisfaction insights.

### Utilization and engagement

- Login frequency and usage trends.
- License utilization rates.
- Heat maps of feature usage.
- Utilization breakdown by department or role.

### Performance and response times

- Page-load times for critical functions.
- Transaction and API response times.
- Report generation durations.
- End-user perceived performance metrics.
